{"name":"大语言模型","slug":"大语言模型","count":3,"postlist":[{"title":"DeepSeek-OCR：基于光学的文本数据压缩","uid":"a8520b94cebc535073bd27494d823aad","slug":"DeepSeek-OCR：基于光学的文本数据压缩","date":"2025-11-12T06:04:56.000Z","updated":"2025-11-12T15:05:01.698Z","comments":true,"path":"api/articles/DeepSeek-OCR：基于光学的文本数据压缩.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":"Abstract在DeepSeek3B-MoE-A570M作为解码器的基础上，设计了一个名为DeepEncoder的OCR编码器模型，通过2D光学映射的方式进行...","permalink":"/post/DeepSeek-OCR：基于光学的文本数据压缩","photos":[],"count_time":{"symbolsCount":"1.8k","symbolsTime":"2 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":3,"path":"api/tags/大语言模型.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},{"title":"基于EVO-2 的基因建模与设计","uid":"08cca76fff38d11f0fe80e9d5364700c","slug":"基于EVO-2-的基因建模与设计","date":"2025-02-25T07:50:36.000Z","updated":"2025-11-12T15:30:36.744Z","comments":true,"path":"api/articles/基于EVO-2-的基因建模与设计.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":"IntroductionEvo 2 是一个生物基础模型，使用涵盖所有可观察的进化物种的代表性快照基因组数据进行训练。相比于特定任务的优化，Evo 2更强调通用能...","permalink":"/post/基于EVO-2-的基因建模与设计","photos":[],"count_time":{"symbolsCount":892,"symbolsTime":"1 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":3,"path":"api/tags/大语言模型.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},{"title":"LLM的微调方法","uid":"d9c45824ce239536b406f9198691a48a","slug":"LLM的微调方法","date":"2025-02-10T18:26:30.000Z","updated":"2025-02-25T07:45:29.000Z","comments":true,"path":"api/articles/LLM的微调方法.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":"Supervised Fine-Tuning | 监督微调监督微调是大模型微调的常用技术，是通过使用带标签的数据集来微调预训练模型的方法，用于特定任务下的应用。...","permalink":"/post/LLM的微调方法","photos":[],"count_time":{"symbolsCount":"2.8k","symbolsTime":"3 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":3,"path":"api/tags/大语言模型.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}]}