{"title":"LLM的微调方法","uid":"d9c45824ce239536b406f9198691a48a","slug":"LLM的微调方法","date":"2025-02-10T18:26:30.000Z","updated":"2025-02-11T04:41:33.412Z","comments":true,"path":"api/articles/LLM的微调方法.json","keywords":"记录, 学习, ClaRnS","cover":[],"content":"<h1 id=\"Supervised-Fine-Tuning-监督微调\"><a href=\"#Supervised-Fine-Tuning-监督微调\" class=\"headerlink\" title=\"Supervised Fine-Tuning | 监督微调\"></a>Supervised Fine-Tuning | 监督微调</h1><p>监督微调是大模型微调的常用技术，是通过使用带标签的数据集来微调预训练模型的方法，用于特定任务下的应用。</p>\n<p>预训练模型通常在无监督数据集上训练，以NTP形式的LLM为例，预训练的过程中只会输入大量文本让模型进行预测，从而提高模型对于语言的基本结构和语义的了解。</p>\n<p>在SFT的过程中，需要使用特定任务的标注数据集对模型进行训练，这些数据集会包含输入和对应的输出标签。从而达到优化模型在特定任务上的表现的目的。</p>\n<p>常见的监督微调方法包括全参微调、部分参数微调(LoRA)、冻结监督微调等。</p>\n<h3 id=\"全参微调\"><a href=\"#全参微调\" class=\"headerlink\" title=\"全参微调\"></a>全参微调</h3><p>全参微调意即将模型的全部参数都纳入梯度计算，这种方式的效果最佳，但需要的算力与模型的预训练相当。</p>\n<h3 id=\"部分参数微调\"><a href=\"#部分参数微调\" class=\"headerlink\" title=\"部分参数微调\"></a>部分参数微调</h3><p>部分参数微调通过只调整一部分参数进行模型的调整。</p>\n<h4 id=\"Low-Rank-Adaption\"><a href=\"#Low-Rank-Adaption\" class=\"headerlink\" title=\"Low-Rank Adaption\"></a>Low-Rank Adaption</h4><p>LoRA 通过引入一个低秩分解的矩阵，将原始的密集参数矩阵分解为两个低秩矩阵的乘积，从而大幅减少微调过程的参数量。</p>\n<p><a href=\"https://arxiv.org/abs/2012.13255\">Aghajanyan</a>的研究表明：预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果。同时在预训练后，越大的模型有越小的内在维度，这也解释了为何大模型都拥有很好的few-shot能力。因此LoRA的作者也认为，参数更新的过程也存在一个‘内在秩’，对于预训练权重矩阵$W_0 \\in \\R^{d \\times k}$，有：<br>$$<br>W_0 + \\Delta W &#x3D; W_0 + BA \\text{  , 其中} B\\in \\R^{d \\times r}, A \\in \\R^{r\\times k} \\text{以及} r \\ll min(d, k)<br>$$</p>\n<p>训练过程中冻结参数$W_0$，仅训练$A$和$B$里面的参数，则如图所示，对于$h&#x3D;W_0 x$，前向传播变为：<br>$$<br>h &#x3D; W_0 x + \\Delta Wx &#x3D; W_0 x + BAx<br>$$</p>\n<p><img src=\"/../gallery/ml/LoRA.png\" alt=\"LoRA\"></p>\n<p>Transformer最核心的参数矩阵有三个：$W_q, W_k, W_v$，LoRA应用到多个参数举证时的效果更好，如下表所示：<br><img src=\"/../gallery/ml/LoRA_Res.png\" alt=\"LoRA Results\"></p>\n<p>当作用到多个参数矩阵时，即使内在秩为2也可以保证模型微调得到不错的效果。</p>\n<p>在训练过程中，低秩的适应矩阵$\\Delta W$仅仅放大了对下游任务有用的特征，而不是预训练模型中的主要特征。</p>\n<h4 id=\"Quantized-Low-Rank-Adaption\"><a href=\"#Quantized-Low-Rank-Adaption\" class=\"headerlink\" title=\"Quantized Low-Rank Adaption\"></a>Quantized Low-Rank Adaption</h4><p>QLoRA 是LoRA的量化版本，它通过将模型的权重量化为低精度格式从而减少内存需求。</p>\n<p>QLoRA 的主要思路包括以下几点：</p>\n<ul>\n<li>量化模型：将原始模型的权重量化为更低的数值精度（例如 4 位浮点数，FP4），显著减少内存占用。</li>\n<li>冻结量化权重：微调过程中，模型的量化权重保持冻结，不会更新。</li>\n<li>使用 LoRA 进行适配：在模型的线性层中引入 LoRA 模块（低秩矩阵），微调这些模块来适配下游任务。</li>\n</ul>\n<h3 id=\"冻结参数微调\"><a href=\"#冻结参数微调\" class=\"headerlink\" title=\"冻结参数微调\"></a>冻结参数微调</h3><p>冻结参数的核心是设置模型参数的<code>requires_grad</code>为<code>False</code></p>\n<h5 id=\"使用Transformers库对bert进行冻结参数的LoRA微调\"><a href=\"#使用Transformers库对bert进行冻结参数的LoRA微调\" class=\"headerlink\" title=\"使用Transformers库对bert进行冻结参数的LoRA微调\"></a>使用Transformers库对bert进行冻结参数的LoRA微调</h5><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki vitesse-dark\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #4D9375\">from</span><span style=\"color: #DBD7CAEE\"> transformers </span><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> AutoTokenizer</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> AutoModelForCausalLM</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> LoraConfig</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> get_peft_model</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">from</span><span style=\"color: #DBD7CAEE\"> peft </span><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> TaskType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 加载预训练模型和分词器</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">tokenizer </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> AutoTokenizer</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">from_pretrained</span><span style=\"color: #666666\">(</span><span style=\"color: #C98A7D99\">&quot;</span><span style=\"color: #C98A7D\">bert-base-uncased</span><span style=\"color: #C98A7D99\">&quot;</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> AutoModelForCausalLM</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">from_pretrained</span><span style=\"color: #666666\">(</span><span style=\"color: #C98A7D99\">&quot;</span><span style=\"color: #C98A7D\">bert-base-uncased</span><span style=\"color: #C98A7D99\">&quot;</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 冻结模型的所有参数</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">for</span><span style=\"color: #DBD7CAEE\"> param </span><span style=\"color: #4D9375\">in</span><span style=\"color: #DBD7CAEE\"> model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">parameters</span><span style=\"color: #666666\">():</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    param</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">requires_grad </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> </span><span style=\"color: #4D9375\">False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 配置 LoRA</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">config </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> LoraConfig</span><span style=\"color: #666666\">(</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    </span><span style=\"color: #BD976A\">task_type</span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\">TaskType</span><span style=\"color: #666666\">.</span><span style=\"color: #C99076\">CAUSAL_LM</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 任务类型</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    </span><span style=\"color: #BD976A\">target_modules</span><span style=\"color: #666666\">=</span><span style=\"color: #C98A7D99\">&quot;</span><span style=\"color: #C98A7D\">.*query_key_value</span><span style=\"color: #C98A7D99\">&quot;</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 目标模块</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    </span><span style=\"color: #BD976A\">modules_to_save</span><span style=\"color: #666666\">=[</span><span style=\"color: #C98A7D99\">&quot;</span><span style=\"color: #C98A7D\">word_embeddings</span><span style=\"color: #C98A7D99\">&quot;</span><span style=\"color: #666666\">]</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 需要保存的模块</span></span>\n<span class=\"line\"><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 获取 LoRA 模型</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> get_peft_model</span><span style=\"color: #666666\">(</span><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> config</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 打印可训练参数</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">print_trainable_parameters</span><span style=\"color: #666666\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 微调模型（假设已有训练代码）</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># trainer.train()</span></span></code></pre></div><h5 id=\"使用Pytorch对ResNet18进行冻结参数微调\"><a href=\"#使用Pytorch对ResNet18进行冻结参数微调\" class=\"headerlink\" title=\"使用Pytorch对ResNet18进行冻结参数微调\"></a>使用Pytorch对ResNet18进行冻结参数微调</h5><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki vitesse-dark\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> torch</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> torch</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">nn </span><span style=\"color: #4D9375\">as</span><span style=\"color: #DBD7CAEE\"> nn</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> torchvision</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">models </span><span style=\"color: #4D9375\">as</span><span style=\"color: #DBD7CAEE\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 加载预训练的 ResNet18 模型</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> models</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">resnet18</span><span style=\"color: #666666\">(</span><span style=\"color: #BD976A\">pretrained</span><span style=\"color: #666666\">=</span><span style=\"color: #4D9375\">True</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 冻结所有参数</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">for</span><span style=\"color: #DBD7CAEE\"> param </span><span style=\"color: #4D9375\">in</span><span style=\"color: #DBD7CAEE\"> model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">parameters</span><span style=\"color: #666666\">():</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    param</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">requires_grad </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> </span><span style=\"color: #4D9375\">False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 替换最后一层（全连接层）</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">fc </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> nn</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">Linear</span><span style=\"color: #666666\">(</span><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">fc</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">in_features</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> </span><span style=\"color: #4C9A91\">4</span><span style=\"color: #666666\">)</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 假设我们有 4 个分类</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 打印可训练参数</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">for</span><span style=\"color: #DBD7CAEE\"> name</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> param </span><span style=\"color: #4D9375\">in</span><span style=\"color: #DBD7CAEE\"> model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">named_parameters</span><span style=\"color: #666666\">():</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    </span><span style=\"color: #4D9375\">if</span><span style=\"color: #DBD7CAEE\"> param</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">requires_grad</span><span style=\"color: #666666\">:</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">        </span><span style=\"color: #B8A965\">print</span><span style=\"color: #666666\">(</span><span style=\"color: #DBD7CAEE\">name</span><span style=\"color: #666666\">)</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 只会打印最后一层的参数</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 定义损失函数和优化器</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">criterion </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> nn</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">CrossEntropyLoss</span><span style=\"color: #666666\">()</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">optimizer </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> torch</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">optim</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">Adam</span><span style=\"color: #666666\">(</span><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">fc</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">parameters</span><span style=\"color: #666666\">(),</span><span style=\"color: #DBD7CAEE\"> </span><span style=\"color: #BD976A\">lr</span><span style=\"color: #666666\">=</span><span style=\"color: #4C9A91\">0.001</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 微调模型（假设已有训练代码）</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># for inputs, labels in dataloader:</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     outputs = model(inputs)</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     loss = criterion(outputs, labels)</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     loss.backward()</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     optimizer.step()</span></span></code></pre></div><h1 id=\"Reinforcement-Learning-from-Human-Feedback-以强化学习方式依据人类反馈优化语言模型\"><a href=\"#Reinforcement-Learning-from-Human-Feedback-以强化学习方式依据人类反馈优化语言模型\" class=\"headerlink\" title=\"Reinforcement Learning from Human Feedback | 以强化学习方式依据人类反馈优化语言模型\"></a>Reinforcement Learning from Human Feedback | 以强化学习方式依据人类反馈优化语言模型</h1><p><code>RLHF 的思想：使用强化学习的方式直接优化带有人类反馈的语言模型。RLHF 使得在一般文本数据语料库上训练的语言模型能和复杂的人类价值观对齐。</code></p>\n<p>RLHF设计了多个模型和不同训练阶段，主要可以分解为三个步骤：</p>\n<ol>\n<li>预训练一个语言模型</li>\n<li>聚合问答数据并训练一个奖励模型</li>\n<li>用强化学习方式微调语言模型</li>\n</ol>\n<p>这三个步骤的实施可查看<a href=\"https://huggingface.co/blog/zh/rlhf\">HuggingFace 关于RLHF的技术博客</a>，不同的模型厂商对于实现的方式基本大同小异，数据的组织和处理方式不同也让模型性能产生了一些区别。</p>\n","feature":true,"text":"Supervised Fine-Tuning | 监督微调监督微调是大模型微调的常用技术，是通过使用带标签的数据集来微调预训练模型的方法，用于特定任务下的应用。...","permalink":"/post/LLM的微调方法","photos":[],"count_time":{"symbolsCount":"3k","symbolsTime":"3 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":1,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":1,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":8,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":2,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":1,"path":"api/tags/大语言模型.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Supervised-Fine-Tuning-%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">Supervised Fine-Tuning | 监督微调</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%85%A8%E5%8F%82%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">全参微调</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">部分参数微调</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Low-Rank-Adaption\"><span class=\"toc-text\">Low-Rank Adaption</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Quantized-Low-Rank-Adaption\"><span class=\"toc-text\">Quantized Low-Rank Adaption</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%86%BB%E7%BB%93%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">冻结参数微调</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8Transformers%E5%BA%93%E5%AF%B9bert%E8%BF%9B%E8%A1%8C%E5%86%BB%E7%BB%93%E5%8F%82%E6%95%B0%E7%9A%84LoRA%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">使用Transformers库对bert进行冻结参数的LoRA微调</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8Pytorch%E5%AF%B9ResNet18%E8%BF%9B%E8%A1%8C%E5%86%BB%E7%BB%93%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">使用Pytorch对ResNet18进行冻结参数微调</span></a></li></ol></li></ol></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Reinforcement-Learning-from-Human-Feedback-%E4%BB%A5%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F%E4%BE%9D%E6%8D%AE%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E4%BC%98%E5%8C%96%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">Reinforcement Learning from Human Feedback | 以强化学习方式依据人类反馈优化语言模型</span></a></li></ol>","author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"聚类作业","uid":"964e6c226f117edbf258c81184a470c0","slug":"聚类作业","date":"2023-12-25T21:35:50.000Z","updated":"2023-12-27T10:09:43.864Z","comments":true,"path":"api/articles/聚类作业.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":" TIP 1、K均值聚类给定8个数据点：。使用K=2 执行K均值聚类，将8个点分组到簇C1和C2。初始化簇中心分别为A1和A2。 一次K均值聚类迭代后C1和C2...","permalink":"/post/聚类作业","photos":[],"count_time":{"symbolsCount":"1.4k","symbolsTime":"1 mins."},"categories":[{"name":"基础","slug":"基础","count":35,"path":"api/categories/基础.json"},{"name":"机器学习","slug":"基础/机器学习","count":7,"path":"api/categories/基础/机器学习.json"},{"name":"作业","slug":"基础/机器学习/作业","count":3,"path":"api/categories/基础/机器学习/作业.json"}],"tags":[{"name":"作业","slug":"作业","count":26,"path":"api/tags/作业.json"},{"name":"模式识别","slug":"模式识别","count":7,"path":"api/tags/模式识别.json"},{"name":"机器学习","slug":"机器学习","count":8,"path":"api/tags/机器学习.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}