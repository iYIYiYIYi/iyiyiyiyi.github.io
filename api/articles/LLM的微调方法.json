{"title":"LLM的微调方法","uid":"d9c45824ce239536b406f9198691a48a","slug":"LLM的微调方法","date":"2025-02-10T18:26:30.000Z","updated":"2025-02-11T04:48:34.434Z","comments":true,"path":"api/articles/LLM的微调方法.json","keywords":"记录, 学习, ClaRnS","cover":[],"content":"<h1 id=\"Supervised-Fine-Tuning-监督微调\"><a href=\"#Supervised-Fine-Tuning-监督微调\" class=\"headerlink\" title=\"Supervised Fine-Tuning | 监督微调\"></a>Supervised Fine-Tuning | 监督微调</h1><p>监督微调是大模型微调的常用技术，是通过使用带标签的数据集来微调预训练模型的方法，用于特定任务下的应用。</p>\n<p>预训练模型通常在无监督数据集上训练，以NTP形式的LLM为例，预训练的过程中只会输入大量文本让模型进行预测，从而提高模型对于语言的基本结构和语义的了解。</p>\n<p>在SFT的过程中，需要使用特定任务的标注数据集对模型进行训练，这些数据集会包含输入和对应的输出标签。从而达到优化模型在特定任务上的表现的目的。</p>\n<p>常见的监督微调方法包括全参微调、部分参数微调(LoRA)、冻结监督微调等。</p>\n<h3 id=\"全参微调\"><a href=\"#全参微调\" class=\"headerlink\" title=\"全参微调\"></a>全参微调</h3><p>全参微调意即将模型的全部参数都纳入梯度计算，这种方式的效果最佳，但需要的算力与模型的预训练相当。</p>\n<h3 id=\"部分参数微调\"><a href=\"#部分参数微调\" class=\"headerlink\" title=\"部分参数微调\"></a>部分参数微调</h3><p>部分参数微调通过只调整一部分参数进行模型的调整。</p>\n<h4 id=\"Low-Rank-Adaption\"><a href=\"#Low-Rank-Adaption\" class=\"headerlink\" title=\"Low-Rank Adaption\"></a>Low-Rank Adaption</h4><p>LoRA 通过引入一个低秩分解的矩阵，将原始的密集参数矩阵分解为两个低秩矩阵的乘积，从而大幅减少微调过程的参数量。</p>\n<p><a href=\"https://arxiv.org/abs/2012.13255\">Aghajanyan</a>的研究表明：预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果。同时在预训练后，越大的模型有越小的内在维度，这也解释了为何大模型都拥有很好的few-shot能力。因此LoRA的作者也认为，参数更新的过程也存在一个‘内在秩’，对于预训练权重矩阵<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"11.784ex\" height=\"2.755ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -967.8 5208.3 1217.8\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"30\" d=\"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(1658.3,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\"></path></g><g data-mml-node=\"msup\" transform=\"translate(2603.1,0)\"><g data-mml-node=\"mtext\" fill=\"red\" stroke=\"red\"><path data-c=\"5C\" d=\"M56 731Q56 740 62 745T75 750Q85 750 92 740Q96 733 270 255T444 -231Q444 -239 438 -244T424 -250Q414 -250 407 -240Q404 -236 230 242T56 731Z\"></path><path data-c=\"52\" d=\"M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z\" transform=\"translate(500,0)\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(1269,477.1) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(520,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1298,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g></g></g></svg></mjx-container>，有：<br><mjx-container class=\"MathJax\" jax=\"SVG\" display=\"true\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"68.579ex\" height=\"2.755ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -967.8 30311.7 1217.8\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"30\" d=\"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(1602.8,0)\"><path data-c=\"2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2603,0)\"><path data-c=\"394\" d=\"M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3436,0)\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(4761.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(5817.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"30\" d=\"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(7420.3,0)\"><path data-c=\"2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(8420.6,0)\"><path data-c=\"1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9179.6,0)\"><path data-c=\"1D434\" d=\"M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z\"></path></g><g data-mml-node=\"mtext\" transform=\"translate(9929.6,0)\"><path data-c=\"A0\" d=\"\"></path><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\" transform=\"translate(250,0)\"></path><path data-c=\"20\" d=\"\" transform=\"translate(528,0)\"></path><text data-variant=\"normal\" transform=\"translate(778,0) scale(1,-1)\" font-size=\"884px\" font-family=\"serif\">其</text><text data-variant=\"normal\" transform=\"translate(1778,0) scale(1,-1)\" font-size=\"884px\" font-family=\"serif\">中</text></g><g data-mml-node=\"mi\" transform=\"translate(12707.6,0)\"><path data-c=\"1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(13744.3,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\"></path></g><g data-mml-node=\"msup\" transform=\"translate(14689.1,0)\"><g data-mml-node=\"mtext\" fill=\"red\" stroke=\"red\"><path data-c=\"5C\" d=\"M56 731Q56 740 62 745T75 750Q85 750 92 740Q96 733 270 255T444 -231Q444 -239 438 -244T424 -250Q414 -250 407 -240Q404 -236 230 242T56 731Z\"></path><path data-c=\"52\" d=\"M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z\" transform=\"translate(500,0)\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(1269,477.1) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(520,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1298,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g></g></g><g data-mml-node=\"mo\" transform=\"translate(17244.8,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(17689.5,0)\"><path data-c=\"1D434\" d=\"M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(18717.3,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\"></path></g><g data-mml-node=\"msup\" transform=\"translate(19662.1,0)\"><g data-mml-node=\"mtext\" fill=\"red\" stroke=\"red\"><path data-c=\"5C\" d=\"M56 731Q56 740 62 745T75 750Q85 750 92 740Q96 733 270 255T444 -231Q444 -239 438 -244T424 -250Q414 -250 407 -240Q404 -236 230 242T56 731Z\"></path><path data-c=\"52\" d=\"M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z\" transform=\"translate(500,0)\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(1269,477.1) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(451,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1229,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g><g data-mml-node=\"mtext\" transform=\"translate(22218.5,0)\"><text data-variant=\"normal\" transform=\"scale(1,-1)\" font-size=\"884px\" font-family=\"serif\">以</text><text data-variant=\"normal\" transform=\"translate(1000,0) scale(1,-1)\" font-size=\"884px\" font-family=\"serif\">及</text></g><g data-mml-node=\"mi\" transform=\"translate(24218.5,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(24947.3,0)\"><path data-c=\"226A\" d=\"M639 -48Q639 -54 634 -60T619 -67H618Q612 -67 536 -26Q430 33 329 88Q61 235 59 239Q56 243 56 250T59 261Q62 266 336 415T615 567L619 568Q622 567 625 567Q639 562 639 548Q639 540 633 534Q632 532 374 391L117 250L374 109Q632 -32 633 -34Q639 -40 639 -48ZM944 -48Q944 -54 939 -60T924 -67H923Q917 -67 841 -26Q735 33 634 88Q366 235 364 239Q361 243 361 250T364 261Q367 266 641 415T920 567L924 568Q927 567 930 567Q944 562 944 548Q944 540 938 534Q937 532 679 391L422 250L679 109Q937 -32 938 -34Q944 -40 944 -48Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(26225.1,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(27103.1,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(27448.1,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(28048.1,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(28437.1,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(28957.1,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(29401.7,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(29922.7,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"></path></g></g></g></svg></mjx-container></p>\n<p>训练过程中冻结参数<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.375ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.123ex\" height=\"1.92ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 1380.6 848.6\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"30\" d=\"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z\"></path></g></g></g></g></svg></mjx-container>，仅训练<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.697ex\" height=\"1.62ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -716 750 716\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D434\" d=\"M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z\"></path></g></g></g></svg></mjx-container>和<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.717ex\" height=\"1.545ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 759 683\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"></path></g></g></g></svg></mjx-container>里面的参数，则如图所示，对于<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.375ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"8.738ex\" height=\"1.945ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 3862.1 859.6\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(853.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(1909.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"30\" d=\"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z\"></path></g></g><g data-mml-node=\"mi\" transform=\"translate(3290.1,0)\"><path data-c=\"1D465\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"></path></g></g></g></svg></mjx-container>，前向传播变为：<br><mjx-container class=\"MathJax\" jax=\"SVG\" display=\"true\"><svg style=\"vertical-align: -0.375ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"31.962ex\" height=\"1.994ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -716 14127.1 881.6\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(853.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(1909.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"30\" d=\"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z\"></path></g></g><g data-mml-node=\"mi\" transform=\"translate(3290.1,0)\"><path data-c=\"1D465\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(4084.3,0)\"><path data-c=\"2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5084.6,0)\"><path data-c=\"394\" d=\"M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5917.6,0)\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6965.6,0)\"><path data-c=\"1D465\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(7815.3,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(8871.1,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"30\" d=\"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z\"></path></g></g><g data-mml-node=\"mi\" transform=\"translate(10251.7,0)\"><path data-c=\"1D465\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(11045.9,0)\"><path data-c=\"2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12046.1,0)\"><path data-c=\"1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12805.1,0)\"><path data-c=\"1D434\" d=\"M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13555.1,0)\"><path data-c=\"1D465\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"></path></g></g></g></svg></mjx-container></p>\n<p><img src=\"/../gallery/ml/LoRA.png\" alt=\"LoRA\"></p>\n<p>Transformer最核心的参数矩阵有三个：<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.65ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"11.328ex\" height=\"2.195ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 5007 970.2\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D45E\" d=\"M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(1352.3,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(1796.9,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(3192.3,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(3637,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D463\" d=\"M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z\"></path></g></g></g></g></svg></mjx-container>，LoRA应用到多个参数举证时的效果更好，如下表所示：<br><img src=\"/../gallery/ml/LoRA_Res.png\" alt=\"LoRA Results\"></p>\n<p>当作用到多个参数矩阵时，即使内在秩为2也可以保证模型微调得到不错的效果。</p>\n<p>在训练过程中，低秩的适应矩阵<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.05ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"4.256ex\" height=\"1.67ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -716 1881 738\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"394\" d=\"M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(833,0)\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g></g></g></svg></mjx-container>仅仅放大了对下游任务有用的特征，而不是预训练模型中的主要特征。</p>\n<h4 id=\"Quantized-Low-Rank-Adaption\"><a href=\"#Quantized-Low-Rank-Adaption\" class=\"headerlink\" title=\"Quantized Low-Rank Adaption\"></a>Quantized Low-Rank Adaption</h4><p>QLoRA 是LoRA的量化版本，它通过将模型的权重量化为低精度格式从而减少内存需求。</p>\n<p>QLoRA 的主要思路包括以下几点：</p>\n<ul>\n<li>量化模型：将原始模型的权重量化为更低的数值精度（例如 4 位浮点数，FP4），显著减少内存占用。</li>\n<li>冻结量化权重：微调过程中，模型的量化权重保持冻结，不会更新。</li>\n<li>使用 LoRA 进行适配：在模型的线性层中引入 LoRA 模块（低秩矩阵），微调这些模块来适配下游任务。</li>\n</ul>\n<h3 id=\"冻结参数微调\"><a href=\"#冻结参数微调\" class=\"headerlink\" title=\"冻结参数微调\"></a>冻结参数微调</h3><p>冻结参数的核心是设置模型参数的<code>requires_grad</code>为<code>False</code></p>\n<h5 id=\"使用Transformers库对bert进行冻结参数的LoRA微调\"><a href=\"#使用Transformers库对bert进行冻结参数的LoRA微调\" class=\"headerlink\" title=\"使用Transformers库对bert进行冻结参数的LoRA微调\"></a>使用Transformers库对bert进行冻结参数的LoRA微调</h5><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki vitesse-dark\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #4D9375\">from</span><span style=\"color: #DBD7CAEE\"> transformers </span><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> AutoTokenizer</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> AutoModelForCausalLM</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> LoraConfig</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> get_peft_model</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">from</span><span style=\"color: #DBD7CAEE\"> peft </span><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> TaskType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 加载预训练模型和分词器</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">tokenizer </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> AutoTokenizer</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">from_pretrained</span><span style=\"color: #666666\">(</span><span style=\"color: #C98A7D99\">\"</span><span style=\"color: #C98A7D\">bert-base-uncased</span><span style=\"color: #C98A7D99\">\"</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> AutoModelForCausalLM</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">from_pretrained</span><span style=\"color: #666666\">(</span><span style=\"color: #C98A7D99\">\"</span><span style=\"color: #C98A7D\">bert-base-uncased</span><span style=\"color: #C98A7D99\">\"</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 冻结模型的所有参数</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">for</span><span style=\"color: #DBD7CAEE\"> param </span><span style=\"color: #4D9375\">in</span><span style=\"color: #DBD7CAEE\"> model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">parameters</span><span style=\"color: #666666\">():</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    param</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">requires_grad </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> </span><span style=\"color: #4D9375\">False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 配置 LoRA</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">config </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> LoraConfig</span><span style=\"color: #666666\">(</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    </span><span style=\"color: #BD976A\">task_type</span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\">TaskType</span><span style=\"color: #666666\">.</span><span style=\"color: #C99076\">CAUSAL_LM</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 任务类型</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    </span><span style=\"color: #BD976A\">target_modules</span><span style=\"color: #666666\">=</span><span style=\"color: #C98A7D99\">\"</span><span style=\"color: #C98A7D\">.*query_key_value</span><span style=\"color: #C98A7D99\">\"</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 目标模块</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    </span><span style=\"color: #BD976A\">modules_to_save</span><span style=\"color: #666666\">=[</span><span style=\"color: #C98A7D99\">\"</span><span style=\"color: #C98A7D\">word_embeddings</span><span style=\"color: #C98A7D99\">\"</span><span style=\"color: #666666\">]</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 需要保存的模块</span></span>\n<span class=\"line\"><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 获取 LoRA 模型</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> get_peft_model</span><span style=\"color: #666666\">(</span><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> config</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 打印可训练参数</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">print_trainable_parameters</span><span style=\"color: #666666\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 微调模型（假设已有训练代码）</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># trainer.train()</span></span></code></pre></div><h5 id=\"使用Pytorch对ResNet18进行冻结参数微调\"><a href=\"#使用Pytorch对ResNet18进行冻结参数微调\" class=\"headerlink\" title=\"使用Pytorch对ResNet18进行冻结参数微调\"></a>使用Pytorch对ResNet18进行冻结参数微调</h5><div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki vitesse-dark\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> torch</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> torch</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">nn </span><span style=\"color: #4D9375\">as</span><span style=\"color: #DBD7CAEE\"> nn</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">import</span><span style=\"color: #DBD7CAEE\"> torchvision</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">models </span><span style=\"color: #4D9375\">as</span><span style=\"color: #DBD7CAEE\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 加载预训练的 ResNet18 模型</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> models</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">resnet18</span><span style=\"color: #666666\">(</span><span style=\"color: #BD976A\">pretrained</span><span style=\"color: #666666\">=</span><span style=\"color: #4D9375\">True</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 冻结所有参数</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">for</span><span style=\"color: #DBD7CAEE\"> param </span><span style=\"color: #4D9375\">in</span><span style=\"color: #DBD7CAEE\"> model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">parameters</span><span style=\"color: #666666\">():</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    param</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">requires_grad </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> </span><span style=\"color: #4D9375\">False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 替换最后一层（全连接层）</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">fc </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> nn</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">Linear</span><span style=\"color: #666666\">(</span><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">fc</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">in_features</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> </span><span style=\"color: #4C9A91\">4</span><span style=\"color: #666666\">)</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 假设我们有 4 个分类</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 打印可训练参数</span></span>\n<span class=\"line\"><span style=\"color: #4D9375\">for</span><span style=\"color: #DBD7CAEE\"> name</span><span style=\"color: #666666\">,</span><span style=\"color: #DBD7CAEE\"> param </span><span style=\"color: #4D9375\">in</span><span style=\"color: #DBD7CAEE\"> model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">named_parameters</span><span style=\"color: #666666\">():</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">    </span><span style=\"color: #4D9375\">if</span><span style=\"color: #DBD7CAEE\"> param</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">requires_grad</span><span style=\"color: #666666\">:</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">        </span><span style=\"color: #B8A965\">print</span><span style=\"color: #666666\">(</span><span style=\"color: #DBD7CAEE\">name</span><span style=\"color: #666666\">)</span><span style=\"color: #DBD7CAEE\">  </span><span style=\"color: #758575DD\"># 只会打印最后一层的参数</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 定义损失函数和优化器</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">criterion </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> nn</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">CrossEntropyLoss</span><span style=\"color: #666666\">()</span></span>\n<span class=\"line\"><span style=\"color: #DBD7CAEE\">optimizer </span><span style=\"color: #666666\">=</span><span style=\"color: #DBD7CAEE\"> torch</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">optim</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">Adam</span><span style=\"color: #666666\">(</span><span style=\"color: #DBD7CAEE\">model</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">fc</span><span style=\"color: #666666\">.</span><span style=\"color: #DBD7CAEE\">parameters</span><span style=\"color: #666666\">(),</span><span style=\"color: #DBD7CAEE\"> </span><span style=\"color: #BD976A\">lr</span><span style=\"color: #666666\">=</span><span style=\"color: #4C9A91\">0.001</span><span style=\"color: #666666\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># 微调模型（假设已有训练代码）</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\"># for inputs, labels in dataloader:</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     outputs = model(inputs)</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     loss = criterion(outputs, labels)</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     loss.backward()</span></span>\n<span class=\"line\"><span style=\"color: #758575DD\">#     optimizer.step()</span></span></code></pre></div><h1 id=\"Reinforcement-Learning-from-Human-Feedback-以强化学习方式依据人类反馈优化语言模型\"><a href=\"#Reinforcement-Learning-from-Human-Feedback-以强化学习方式依据人类反馈优化语言模型\" class=\"headerlink\" title=\"Reinforcement Learning from Human Feedback | 以强化学习方式依据人类反馈优化语言模型\"></a>Reinforcement Learning from Human Feedback | 以强化学习方式依据人类反馈优化语言模型</h1><p><code>RLHF 的思想：使用强化学习的方式直接优化带有人类反馈的语言模型。RLHF 使得在一般文本数据语料库上训练的语言模型能和复杂的人类价值观对齐。</code></p>\n<p>RLHF设计了多个模型和不同训练阶段，主要可以分解为三个步骤：</p>\n<ol>\n<li>预训练一个语言模型</li>\n<li>聚合问答数据并训练一个奖励模型</li>\n<li>用强化学习方式微调语言模型</li>\n</ol>\n<p>这三个步骤的实施可查看<a href=\"https://huggingface.co/blog/zh/rlhf\">HuggingFace 关于RLHF的技术博客</a>，不同的模型厂商对于实现的方式基本大同小异，数据的组织和处理方式不同也让模型性能产生了一些区别。</p>\n","feature":true,"text":"Supervised Fine-Tuning | 监督微调监督微调是大模型微调的常用技术，是通过使用带标签的数据集来微调预训练模型的方法，用于特定任务下的应用。...","permalink":"/post/LLM的微调方法","photos":[],"count_time":{"symbolsCount":"2.8k","symbolsTime":"3 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":1,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":1,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":8,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":2,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":1,"path":"api/tags/大语言模型.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Supervised-Fine-Tuning-%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">Supervised Fine-Tuning | 监督微调</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%85%A8%E5%8F%82%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">全参微调</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">部分参数微调</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Low-Rank-Adaption\"><span class=\"toc-text\">Low-Rank Adaption</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Quantized-Low-Rank-Adaption\"><span class=\"toc-text\">Quantized Low-Rank Adaption</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%86%BB%E7%BB%93%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">冻结参数微调</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8Transformers%E5%BA%93%E5%AF%B9bert%E8%BF%9B%E8%A1%8C%E5%86%BB%E7%BB%93%E5%8F%82%E6%95%B0%E7%9A%84LoRA%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">使用Transformers库对bert进行冻结参数的LoRA微调</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8Pytorch%E5%AF%B9ResNet18%E8%BF%9B%E8%A1%8C%E5%86%BB%E7%BB%93%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">使用Pytorch对ResNet18进行冻结参数微调</span></a></li></ol></li></ol></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Reinforcement-Learning-from-Human-Feedback-%E4%BB%A5%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F%E4%BE%9D%E6%8D%AE%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E4%BC%98%E5%8C%96%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">Reinforcement Learning from Human Feedback | 以强化学习方式依据人类反馈优化语言模型</span></a></li></ol>","author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"聚类作业","uid":"964e6c226f117edbf258c81184a470c0","slug":"聚类作业","date":"2023-12-25T21:35:50.000Z","updated":"2023-12-27T10:09:43.864Z","comments":true,"path":"api/articles/聚类作业.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":" TIP 1、K均值聚类给定8个数据点：。使用K=2 执行K均值聚类，将8个点分组到簇C1和C2。初始化簇中心分别为A1和A2。 一次K均值聚类迭代后C1和C2...","permalink":"/post/聚类作业","photos":[],"count_time":{"symbolsCount":"1.4k","symbolsTime":"1 mins."},"categories":[{"name":"基础","slug":"基础","count":35,"path":"api/categories/基础.json"},{"name":"机器学习","slug":"基础/机器学习","count":7,"path":"api/categories/基础/机器学习.json"},{"name":"作业","slug":"基础/机器学习/作业","count":3,"path":"api/categories/基础/机器学习/作业.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":8,"path":"api/tags/机器学习.json"},{"name":"作业","slug":"作业","count":26,"path":"api/tags/作业.json"},{"name":"模式识别","slug":"模式识别","count":7,"path":"api/tags/模式识别.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}