{"title":"DeepSeek-OCR：基于光学的文本数据压缩","uid":"a8520b94cebc535073bd27494d823aad","slug":"DeepSeek-OCR：基于光学的文本数据压缩","date":"2025-11-12T06:04:56.000Z","updated":"2025-11-12T15:05:01.698Z","comments":true,"path":"api/articles/DeepSeek-OCR：基于光学的文本数据压缩.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","content":"<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>在DeepSeek3B-MoE-A570M作为解码器的基础上，设计了一个名为DeepEncoder的OCR编码器模型，通过2D光学映射的方式进行上下文的压缩。DeepEncoder编码得到的二维图像tokens比文本tokens少十倍以上，并且在压缩比为10时，解码的准确率可以达到97%；即使压缩比达到20+，解码的准确率依然有60%。</p>\n<h1 id=\"Vision-Encoder\"><a href=\"#Vision-Encoder\" class=\"headerlink\" title=\"Vision Encoder\"></a>Vision Encoder</h1><p>DeepEncoder是基于ViT的视觉模型，其基础是由Meta提出的SAM(Segement Anything Model)。ViT(Vision Transformer) 是Transformer架构的图像变体，通过将图片切分成多个小的像素patch(4<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0.02ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.76ex\" height=\"1.09ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -491 778 482\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g></g></g></svg></mjx-container>4、5<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0.02ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"1.76ex\" height=\"1.09ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -491 778 482\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g></g></g></svg></mjx-container>5 …)组合成线性序列输入模型，从而解决Transformer只能处理一维序列数据的问题。</p>\n<p><img src=\"/../gallery/ml/vision_encoders.png\" alt=\"Vision Encoders\"></p>\n<p>在ViT的基础上所衍生出来的视觉编码器主要有两个主流模型，其一是由Meta所提出的SAM分割模型，该模型使用文本+分割+像素区域+物体识别的方法，可以通过文本从图像中准确地提取物体的像素边界。其二是由OpenAI所开发的CLIP模型，该模型的输入是图像，输出则是经过识别后的图像物体。</p>\n<h1 id=\"Vision-LLMS\"><a href=\"#Vision-LLMS\" class=\"headerlink\" title=\"Vision LLMS\"></a>Vision LLMS</h1><p>在现有的支持视觉的大语言模型中，主要有三种主流的编码方案可以将图像编码并输入到LLM中。</p>\n<p>其一（Vary、DeepSeekVL、…）是通过多个不同分辨率的SAM编码器组成的混合编码器，这种方式支持多种预设分辨率图像，但是缺少对于极端分辨率图像的处理能力，同时由于编码器的复杂性，部署起来比较困难。</p>\n<p>第二种（InternVL、DeepSeekVL2、…）则是将图像进行分割，每一个子图独立处理并在LLM中进行整合，这种方式支持大分辨率图像，但是对图像的切分导致模型缺少对图像整体的理解，同时由于一张图被切分为多个tile，编码器会产生大量vision token。</p>\n<p>第三种方式（Qwen2、Qwen2.5、Qwen3、…）则是通过将图像进行切分后打包成长序列，并通过子图的原始位置对其进行顺序编码的方式既兼顾了大分辨率图像的处理问题，也考虑到了编码器对图像的全局视野问题。然而这种方式需要编码器支持更长的序列长度，并由此而导致了模型的推理效率低下的问题。</p>\n<p><img src=\"/../gallery/ml/vision_llms.png\" alt=\"Vision LLMs\"></p>\n<h1 id=\"DeepSeek-OCR-Model\"><a href=\"#DeepSeek-OCR-Model\" class=\"headerlink\" title=\"DeepSeek-OCR Model\"></a>DeepSeek-OCR Model</h1><p>DeepSeek-OCR的核心DeepEncoder为了解决上述三种方案中存在的问题，提出了一种新的基于SAM分割模型以及CLIP图文模型的混合结构模型。在将输入图像切分为patches之后，DeepEncoder首先通过SAM模型对图像进行切分提取其中包含内容的区块，随后通过CLIP对分割后的区块进行描述，最后生成的文本将作为LLM的输入。</p>\n<p>随后，DeepEncoder编码得到的结果将通过LLM进行重建，将其恢复为文本内容，此时便完成了输入文档从图像到文本的变换。</p>\n<p><img src=\"/../gallery/ml/deepseek_ocr.png\" alt=\"DeepSeek-OCR\"></p>\n<p>DeepEncoder的训练数据是通过HTML标注的，文档中的表格和图表会被转换为HTML表格标签，便于LLM在重建时进行图表内容的恢复。几何图形数据会被使用json进行描述保存，其中的顶点位置、边的位置、符号位置等均通过json进行结构化存储。</p>\n<h1 id=\"Model-Training\"><a href=\"#Model-Training\" class=\"headerlink\" title=\"Model Training\"></a>Model Training</h1><p>模型的训练分为两个步骤：DeepEncoder的训练和DeepSeek-OCR的训练。</p>\n<ul>\n<li>DeepEncoder的训练:<ul>\n<li>通过NTP的方式进行训练</li>\n<li>使用AdamW优化器以及余弦退火调度器</li>\n<li>训练序列长度为4096</li>\n<li>一共训练了2个epoch，batchsize设置为1280</li>\n</ul>\n</li>\n<li>DeepSeek-OCR的训练<ul>\n<li>使用管道并行的方式进行训练</li>\n<li>使用了20个节点，每个节点包括8张A100-40G</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>DeepSeek-OCR通过DeepEncoder和LLM相结合的混合架构，实现了对扫描文档的光学压缩以及文本重建，并且保持了重建文本的高准确性。DeepEncoder有望在LLM的上下文压缩方向进行应用，实现LLM上下文长度的进一步拓展。同时，目前的DeepSeek-OCR也可以通过将扫描文档转换为文本文件的形式为LLM的训练生成训练数据，在单张A100-40G的GPU上生成速度可以达到20w页每天。</p>\n","feature":true,"text":"Abstract在DeepSeek3B-MoE-A570M作为解码器的基础上，设计了一个名为DeepEncoder的OCR编码器模型，通过2D光学映射的方式进行...","permalink":"/post/DeepSeek-OCR：基于光学的文本数据压缩","photos":[],"count_time":{"symbolsCount":"1.8k","symbolsTime":"2 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":3,"path":"api/tags/大语言模型.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Abstract\"><span class=\"toc-text\">Abstract</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Vision-Encoder\"><span class=\"toc-text\">Vision Encoder</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Vision-LLMS\"><span class=\"toc-text\">Vision LLMS</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#DeepSeek-OCR-Model\"><span class=\"toc-text\">DeepSeek-OCR Model</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Model-Training\"><span class=\"toc-text\">Model Training</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Conclusion\"><span class=\"toc-text\">Conclusion</span></a></li></ol>","author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"AlphaGenome：多分辨率基因组模型","uid":"622fbcd79d3811e5898cc0046d22e71d","slug":"AlphaGenome：多分辨率基因组模型","date":"2025-11-13T07:02:30.000Z","updated":"2025-11-13T15:41:57.875Z","comments":true,"path":"api/articles/AlphaGenome：多分辨率基因组模型.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":"AbstractAlphaGenome 模型将长 DNA 序列作为输入（多达 100 万个字母，也称为碱基对），并预测其调控活动的数千种分子特性。它还可以通过比...","permalink":"/post/AlphaGenome：多分辨率基因组模型","photos":[],"count_time":{"symbolsCount":"1.4k","symbolsTime":"1 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"基于EVO-2 的基因建模与设计","uid":"08cca76fff38d11f0fe80e9d5364700c","slug":"基于EVO-2-的基因建模与设计","date":"2025-02-25T07:50:36.000Z","updated":"2025-11-12T15:30:36.744Z","comments":true,"path":"api/articles/基于EVO-2-的基因建模与设计.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":"IntroductionEvo 2 是一个生物基础模型，使用涵盖所有可观察的进化物种的代表性快照基因组数据进行训练。相比于特定任务的优化，Evo 2更强调通用能...","permalink":"/post/基于EVO-2-的基因建模与设计","photos":[],"count_time":{"symbolsCount":892,"symbolsTime":"1 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":3,"path":"api/tags/大语言模型.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}