{"title":"基于EVO-2 的基因建模与设计","uid":"08cca76fff38d11f0fe80e9d5364700c","slug":"基于EVO-2-的基因建模与设计","date":"2025-02-25T07:50:36.000Z","updated":"2025-11-12T15:30:36.744Z","comments":true,"path":"api/articles/基于EVO-2-的基因建模与设计.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","content":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>Evo 2 是一个生物基础模型，使用涵盖所有可观察的进化物种的代表性快照基因组数据进行训练。相比于特定任务的优化，Evo 2更强调通用能力，并且在从分子到基因组甚至到所有生命领域都具有健壮的预测和生成能力。</p>\n<h1 id=\"Model-Architecture\"><a href=\"#Model-Architecture\" class=\"headerlink\" title=\"Model Architecture\"></a>Model Architecture</h1><p>evo2采用StripedHyena2作为基础模型，通过多层模型的堆叠增加参数量，包含7B和40B参数的两个版本。由于生物信息学数据的高噪声低语义特点，生物模型的设计一直以来存在难点，参数量更大的模型容易陷入噪声，参数量小的模型无法识别模式。StripedHyena2作为一种序列模型，相较Transformer的长序列建模能力更好。</p>\n<p><img src=\"/../gallery/ml/evo2.png\" alt=\"Evo2\"></p>\n<p>Evo2使用到了Sequence Packing的技术对训练进行加速。</p>\n<h1 id=\"Sequence-Packing\"><a href=\"#Sequence-Packing\" class=\"headerlink\" title=\"Sequence Packing\"></a>Sequence Packing</h1><p>序列模型中的序列长度不一致问题导致padding对齐需要产生额外的计算消耗。Sequence Packing即把多个样本或序列拼在一起。拼接后可以减少数据处理的条数，同时减少序列中pad的数目从而减少计算消耗。</p>\n<p>拼接算法通常使用NNLSHP （Non-Negative Least Squares Histogram Packing），该算法以数据长度的直方图为基础，选择直方图中相邻的N个符合Pack大小的长度的数据合并。由于直方图记录了数据的原始信息，因此也可以将数据还原到原来的长度。</p>\n<h1 id=\"Training\"><a href=\"#Training\" class=\"headerlink\" title=\"Training\"></a>Training</h1><p>Evo2分为两个阶段进行训练：预训练（Pretrain）和中期训练（Midtrain），其中训练数据包含DNA序列和RNA序列。</p>\n<p>Pretrain阶段采用NTP的方式进行训练，一共使用到了6.6T个碱基的数据，其中重复出现的碱基片段通过归一化因子降低对模型的影响。预训练阶段使用8192的上下文长度进行训练，其中40B版本的模型先使用1024的上下文长度对模型进行预热，随后将上下文窗口拓展到8192个碱基。</p>\n<p>Midtrain阶段是为了拓展模型的上下文而设计的，通过旋转位置编码（RoPE）和缩小位置索引对token位置进行插值的方法对上下文长度进行进一步的拓展。</p>\n","feature":true,"text":"IntroductionEvo 2 是一个生物基础模型，使用涵盖所有可观察的进化物种的代表性快照基因组数据进行训练。相比于特定任务的优化，Evo 2更强调通用能...","permalink":"/post/基于EVO-2-的基因建模与设计","photos":[],"count_time":{"symbolsCount":892,"symbolsTime":"1 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":3,"path":"api/tags/大语言模型.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Introduction\"><span class=\"toc-text\">Introduction</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Model-Architecture\"><span class=\"toc-text\">Model Architecture</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Sequence-Packing\"><span class=\"toc-text\">Sequence Packing</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Training\"><span class=\"toc-text\">Training</span></a></li></ol>","author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"DeepSeek-OCR：基于光学的文本数据压缩","uid":"a8520b94cebc535073bd27494d823aad","slug":"DeepSeek-OCR：基于光学的文本数据压缩","date":"2025-11-12T06:04:56.000Z","updated":"2025-11-12T15:05:01.698Z","comments":true,"path":"api/articles/DeepSeek-OCR：基于光学的文本数据压缩.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":"Abstract在DeepSeek3B-MoE-A570M作为解码器的基础上，设计了一个名为DeepEncoder的OCR编码器模型，通过2D光学映射的方式进行...","permalink":"/post/DeepSeek-OCR：基于光学的文本数据压缩","photos":[],"count_time":{"symbolsCount":"1.8k","symbolsTime":"2 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":3,"path":"api/tags/大语言模型.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"LLM的微调方法","uid":"d9c45824ce239536b406f9198691a48a","slug":"LLM的微调方法","date":"2025-02-10T18:26:30.000Z","updated":"2025-02-25T07:45:29.000Z","comments":true,"path":"api/articles/LLM的微调方法.json","keywords":"记录, 学习, ClaRnS","cover":"/gallery/ml.png","text":"Supervised Fine-Tuning | 监督微调监督微调是大模型微调的常用技术，是通过使用带标签的数据集来微调预训练模型的方法，用于特定任务下的应用。...","permalink":"/post/LLM的微调方法","photos":[],"count_time":{"symbolsCount":"2.8k","symbolsTime":"3 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":4,"path":"api/categories/机器学习.json"},{"name":"LLM","slug":"机器学习/LLM","count":4,"path":"api/categories/机器学习/LLM.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":11,"path":"api/tags/机器学习.json"},{"name":"人工智能","slug":"人工智能","count":5,"path":"api/tags/人工智能.json"},{"name":"大语言模型","slug":"大语言模型","count":3,"path":"api/tags/大语言模型.json"}],"author":{"name":"ClaRn","slug":"blog-author","avatar":"/gallery/avatar.jpg","link":"/","description":"当你在浪费时间的事情里获得了快乐，那就不是在浪费时间。 ——罗素","socials":{"github":"https://github.com/iYIYiYIYi","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}